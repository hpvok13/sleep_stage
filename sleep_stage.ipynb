{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "VrVPPyDP1lY1",
      "metadata": {
        "id": "VrVPPyDP1lY1"
      },
      "source": [
        "## Resources\n",
        "\n",
        "* [OpenMind Wiki](https://github.mit.edu/MGHPCC/OpenMind/wiki)\n",
        "* [OpenMind Website](openmind.mit.edu)\n",
        "* [OpenMind Jupyter Notebook Tutorial](https://github.mit.edu/MGHPCC/OpenMind/wiki/How-to-use-Jupyter-Notebook-on-OpenMind%3F)\n",
        "* [Accusleep Dataset](https://osf.io/py5eb/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da745c8",
      "metadata": {
        "id": "9da745c8"
      },
      "source": [
        "## Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e41b9a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e41b9a6",
        "outputId": "b4665684-c75f-43b7-aa38-d051040f5550"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c7cdbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52c7cdbd",
        "outputId": "44acc15c-9ef1-45e7-ba7a-107c8e4efc9a"
      },
      "outputs": [],
      "source": [
        "# Check correct hostname\n",
        "!hostname"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ebce31",
      "metadata": {
        "id": "f5ebce31"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c879fa4",
      "metadata": {
        "id": "5c879fa4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import ProjectConfiguration\n",
        "from sklearn.metrics import classification_report, balanced_accuracy_score, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import resample\n",
        "from mne.filter import filter_data\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "from types import SimpleNamespace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a9fda2",
      "metadata": {
        "id": "80a9fda2"
      },
      "source": [
        "## Helper Functions/Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f65aaadd",
      "metadata": {
        "id": "f65aaadd"
      },
      "outputs": [],
      "source": [
        "def downsample(x, sf, new_sf):\n",
        "    num = int(new_sf / sf * x.shape[-1])\n",
        "    return resample(x, num, axis=-1)\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "class SleepDataset(Dataset):\n",
        "    \"\"\"Dataset class for EEG/EMG sleep data from the Accusleep Dataset (https://osf.io/py5eb/)\n",
        "        Supports \"homemade\" data of the same file/directory format\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir,\n",
        "        raw_sf,\n",
        "        model_sf,\n",
        "        raw_epoch_len,\n",
        "        model_epoch_len,\n",
        "        homemade,\n",
        "        *,\n",
        "        run_preproc=False,\n",
        "        bandpass_freqs=None,\n",
        "        context_window=1,\n",
        "        eeg_idx=0,\n",
        "        emg_idx=5,\n",
        "        random_shift=False,\n",
        "        eeg_transform=None,\n",
        "        emg_transform=None,\n",
        "        target_transform=None,\n",
        "    ):\n",
        "        # Build the paths\n",
        "        all_paths = list(Path(data_dir).glob(\"**/*\"))\n",
        "        all_dirs = sorted([folder for folder in all_paths if folder.is_dir()])\n",
        "        if not all_dirs:\n",
        "            all_dirs = [Path(data_dir)]\n",
        "        assert (\n",
        "            float(raw_sf * raw_epoch_len).is_integer()\n",
        "            and float(model_sf * model_epoch_len).is_integer()\n",
        "        )\n",
        "\n",
        "        # Save attributes\n",
        "        self.window_size = int(model_sf * model_epoch_len)\n",
        "        self.raw_sf = raw_sf\n",
        "        self.model_sf = model_sf\n",
        "        self.raw_epoch_len = raw_epoch_len\n",
        "        self.model_epoch_len = model_epoch_len\n",
        "        assert context_window >= 1\n",
        "        self.context_window = context_window\n",
        "        self.random_shift = random_shift\n",
        "        self.eeg_transform = eeg_transform\n",
        "        self.emg_transform = emg_transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.eegs = []\n",
        "        self.emgs = []\n",
        "        self.labels = []\n",
        "        index_map = []\n",
        "        dir_map = []\n",
        "        weight = torch.tensor([0,0,0])\n",
        "        i_map = 0\n",
        "\n",
        "        # Load data\n",
        "        for dir in all_dirs:\n",
        "            try:\n",
        "                if run_preproc:\n",
        "                    self.preprocess(dir, homemade, raw_sf, model_sf, bandpass_freqs)\n",
        "                eeg, emg, label = self.get_preproc_files(\n",
        "                    dir, homemade, eeg_idx, emg_idx\n",
        "                )\n",
        "            except FileNotFoundError:\n",
        "                continue\n",
        "            self.eegs.append(eeg)\n",
        "            self.emgs.append(emg)\n",
        "            self.labels.append(label)\n",
        "\n",
        "            # Count labels for class weighting\n",
        "            _, counts = label.unique(sorted=True, return_counts=True)\n",
        "            weight += counts\n",
        "\n",
        "            # Mapping from \"global index\" to list and tensor indices\n",
        "            dir_map.append(np.repeat(i_map, len(label) - self.context_window + 1))\n",
        "            index_map.append(np.arange(self.context_window - 1, len(label)))\n",
        "            i_map += 1\n",
        "        if len(dir_map) == 0:\n",
        "            raise FileNotFoundError\n",
        "\n",
        "        # Build index mapping\n",
        "        self.weight = 1 - weight / weight.sum()\n",
        "        dir_map = np.concatenate(dir_map)\n",
        "        index_map = np.concatenate(index_map)\n",
        "        self.index_map = np.column_stack((dir_map, index_map))\n",
        "\n",
        "    def get_preproc_files(self, data_dir, homemade, eeg_idx, emg_idx):\n",
        "        \"\"\"Retrieves preprocessed EEG, EMG, and label data\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): directory in which data is stored\n",
        "            homemade (bool): homemade or Accusleep data\n",
        "            eeg_idx (int): desired eeg probe index in raw homemade data array\n",
        "            emg_idx (int): desired emg probe index in raw homemade data array\n",
        "            a\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: raised if EEG.mat, EMG.mat, or labels.mat do not exist within target_folder\n",
        "\n",
        "        Returns:\n",
        "            eeg_array, emg_array, label_array (torch.Tensor, torch.Tensor, torch.Tensor): EEG, EMG, and label data\n",
        "            as numpy arrays\n",
        "        \"\"\"\n",
        "        if homemade:\n",
        "            eeg_emg_file = os.path.join(data_dir, \"EEG-EMG_preproc.npy\")\n",
        "            label_file = os.path.join(data_dir, \"labels_preproc.npy\")\n",
        "\n",
        "            paths_are_files = list(\n",
        "                map(lambda x: os.path.isfile(x), [eeg_emg_file, label_file])\n",
        "            )\n",
        "            if not all(paths_are_files):\n",
        "                raise FileNotFoundError\n",
        "            # load files\n",
        "            eeg_emg = np.load(eeg_emg_file)\n",
        "            label = np.load(label_file)\n",
        "\n",
        "            eeg, emg = eeg_emg[eeg_idx], eeg_emg[emg_idx]\n",
        "        else:\n",
        "            eeg_file = os.path.join(data_dir, \"EEG_preproc.npy\")\n",
        "            emg_file = os.path.join(data_dir, \"EMG_preproc.npy\")\n",
        "            label_file = os.path.join(data_dir, \"labels_preproc.npy\")\n",
        "\n",
        "            paths_are_files = list(\n",
        "                map(lambda x: os.path.isfile(x), [eeg_file, emg_file, label_file])\n",
        "            )\n",
        "            if not all(paths_are_files):\n",
        "                raise FileNotFoundError\n",
        "            # load files\n",
        "            eeg = np.load(eeg_file)\n",
        "            emg = np.load(emg_file)\n",
        "            label = np.load(label_file)\n",
        "\n",
        "        # Check lengths\n",
        "        assert eeg.shape[0] == emg.shape[0]\n",
        "        num_eeg_samples = eeg.shape[0] / self.window_size\n",
        "        assert num_eeg_samples >= len(label)\n",
        "\n",
        "        eeg = eeg[: len(label) * self.window_size]\n",
        "        emg = emg[: len(label) * self.window_size]\n",
        "        return (\n",
        "            torch.tensor(eeg, dtype=torch.float),\n",
        "            torch.tensor(emg, dtype=torch.float),\n",
        "            torch.tensor(label, dtype=torch.long),\n",
        "        )\n",
        "\n",
        "    def preprocess(\n",
        "        self,\n",
        "        data_dir,\n",
        "        homemade,\n",
        "        raw_sf,\n",
        "        model_sf,\n",
        "        bandpass_freqs,\n",
        "    ):\n",
        "        if homemade:\n",
        "            eeg_emg_file = os.path.join(data_dir, \"EEG-EMG.npy\")\n",
        "            label_file = os.path.join(data_dir, \"labels.npy\")\n",
        "\n",
        "            paths_are_files = list(\n",
        "                map(lambda x: os.path.isfile(x), [eeg_emg_file, label_file])\n",
        "            )\n",
        "            if not all(paths_are_files):\n",
        "                raise FileNotFoundError\n",
        "            # load files\n",
        "            eeg_emg = np.load(eeg_emg_file)\n",
        "            label = np.load(label_file)\n",
        "\n",
        "            if bandpass_freqs:\n",
        "                eeg_emg = filter_data(\n",
        "                    eeg_emg.astype(\"float64\"),\n",
        "                    raw_sf,\n",
        "                    bandpass_freqs[0],\n",
        "                    bandpass_freqs[1],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            if raw_sf != model_sf:\n",
        "                eeg_emg = downsample(eeg_emg, raw_sf, model_sf)\n",
        "\n",
        "            # Scale EEG and EMG\n",
        "            eeg_emg = RobustScaler().fit_transform(eeg_emg.T).T  # type: ignore\n",
        "\n",
        "            num_samples = int(eeg_emg.shape[1] // (model_sf * self.raw_epoch_len))\n",
        "            assert num_samples == len(label)\n",
        "\n",
        "            if self.raw_epoch_len != self.model_epoch_len:\n",
        "                label_new_len = int(\n",
        "                    len(label) * self.raw_epoch_len // self.model_epoch_len\n",
        "                )\n",
        "                eeg_emg = eeg_emg[\n",
        "                    : int(label_new_len * model_sf * self.model_epoch_len)\n",
        "                ]\n",
        "\n",
        "            np.save(os.path.join(data_dir, \"EEG-EMG_preproc.npy\"), eeg_emg)\n",
        "        else:\n",
        "            eeg_file = os.path.join(data_dir, \"EEG.mat\")\n",
        "            emg_file = os.path.join(data_dir, \"EMG.mat\")\n",
        "            label_file = os.path.join(data_dir, \"labels.mat\")\n",
        "\n",
        "            paths_are_files = list(\n",
        "                map(lambda x: os.path.isfile(x), [eeg_file, emg_file, label_file])\n",
        "            )\n",
        "            if not all(paths_are_files):\n",
        "                raise FileNotFoundError\n",
        "            # load files\n",
        "            eeg = loadmat(eeg_file)\n",
        "            emg = loadmat(emg_file)\n",
        "            label = loadmat(label_file)\n",
        "\n",
        "            eeg = np.squeeze(eeg[\"EEG\"])\n",
        "            emg = np.squeeze(emg[\"EMG\"])\n",
        "            label = np.squeeze(label[\"labels\"])\n",
        "\n",
        "            assert eeg.shape[0] == emg.shape[0]\n",
        "\n",
        "            if bandpass_freqs:\n",
        "                eeg = filter_data(\n",
        "                    eeg.astype(\"float64\"),\n",
        "                    raw_sf,\n",
        "                    bandpass_freqs[0],\n",
        "                    bandpass_freqs[1],\n",
        "                    verbose=0,\n",
        "                )\n",
        "                emg = filter_data(\n",
        "                    emg.astype(\"float64\"),\n",
        "                    raw_sf,\n",
        "                    bandpass_freqs[0],\n",
        "                    bandpass_freqs[1],\n",
        "                    verbose=0,\n",
        "                )\n",
        "            if raw_sf != model_sf:\n",
        "                eeg = downsample(eeg, raw_sf, model_sf)\n",
        "                emg = downsample(emg, raw_sf, model_sf)\n",
        "\n",
        "            # Scale EEG and EMG\n",
        "            eeg = RobustScaler().fit_transform(eeg[:, np.newaxis]).squeeze()  # type: ignore\n",
        "            emg = RobustScaler().fit_transform(emg[:, np.newaxis]).squeeze()  # type: ignore\n",
        "\n",
        "            num_samples = int(eeg.shape[0] // (model_sf * self.raw_epoch_len))\n",
        "            assert num_samples == len(label)\n",
        "\n",
        "            if self.raw_epoch_len != self.model_epoch_len:\n",
        "                label_new_len = int(\n",
        "                    len(label) * self.raw_epoch_len // self.model_epoch_len\n",
        "                )\n",
        "                eeg = eeg[: int(label_new_len * model_sf * self.model_epoch_len)]\n",
        "                emg = emg[: int(label_new_len * model_sf * self.model_epoch_len)]\n",
        "\n",
        "            np.save(os.path.join(data_dir, \"EEG_preproc.npy\"), eeg)\n",
        "            np.save(os.path.join(data_dir, \"EMG_preproc.npy\"), emg)\n",
        "\n",
        "        accusleep_dict = {\n",
        "            1: 2,  # \"R\",\n",
        "            2: 0,  # \"W\",\n",
        "            3: 1,  # \"N\"\n",
        "        }\n",
        "        # re-map the label values\n",
        "        label_df = pd.DataFrame({\"label\": label})\n",
        "        label_df[\"label\"] = label_df[\"label\"].map(accusleep_dict)\n",
        "        label = label_df[\"label\"].values\n",
        "\n",
        "        if self.raw_epoch_len != self.model_epoch_len:\n",
        "            label_new = np.zeros(label_new_len, dtype=int)  # type: ignore\n",
        "            for i in range(len(label_new)):\n",
        "                label_new[i] = label[\n",
        "                    int(round(i * self.model_epoch_len / self.raw_epoch_len))\n",
        "                ]\n",
        "            label = label_new\n",
        "\n",
        "        np.save(os.path.join(data_dir, \"labels_preproc.npy\"), label)  # type: ignore\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.index_map.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dir_idx, adjusted_idx = self.index_map[idx]\n",
        "        label = self.labels[dir_idx][adjusted_idx]\n",
        "        if self.random_shift and adjusted_idx < len(self.labels[dir_idx]) - 1:\n",
        "            offset = np.random.randint(self.window_size)\n",
        "        else:\n",
        "            offset = 0\n",
        "        low = (adjusted_idx - self.context_window + 1) * self.window_size + offset\n",
        "        high = (adjusted_idx + 1) * self.window_size + offset\n",
        "        eeg = self.eegs[dir_idx][low:high]\n",
        "        emg = self.emgs[dir_idx][low:high]\n",
        "        if self.eeg_transform:\n",
        "            eeg = self.eeg_transform(eeg)\n",
        "        if self.emg_transform:\n",
        "            emg = self.emg_transform(emg)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        x = torch.stack((eeg, emg), dim=0)\n",
        "        return x, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266a12c4",
      "metadata": {
        "id": "266a12c4"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b722a1",
      "metadata": {
        "id": "54b722a1"
      },
      "outputs": [],
      "source": [
        "params = AttrDict(\n",
        "    # System\n",
        "    threads_per_gpu = 1,\n",
        "    gpus = 4,\n",
        "\n",
        "    # Data\n",
        "    train_data_dir = 'data/24-hour_recordings',\n",
        "    test_data_dir = 'data/4-hour_recordings',\n",
        "    sample_data_dir = 'data/4-hour_recordings/Mouse01/Day1',\n",
        "    homemade_data_dir = 'data/homemade_recordings',\n",
        "    pretrain_dir = 'pretrain',\n",
        "    finetune_dir = 'finetune',\n",
        "    homemade_eeg_idx = 4,\n",
        "    homemade_emg_idx = 5,\n",
        "    model_sf = 128,\n",
        "    accusleep_sf = 512, # Hz\n",
        "    homemade_sf = 1000, # Hz\n",
        "    bandpass_freqs = [1, 64],\n",
        "    run_preproc = False,\n",
        "    homemade_epoch_len = 2.0, # sec\n",
        "    accusleep_epoch_len = 2.5, # sec\n",
        "    model_epoch_len = 2.5, # sec\n",
        "    context_window = 3,\n",
        "    random_shift = False,\n",
        "    eeg_transform = None,\n",
        "    emg_transform = None,\n",
        "    target_transform = None,\n",
        "\n",
        "    # Model\n",
        "    in_dim = 2,\n",
        "    out_dim = 3,\n",
        "    embed_dim = 32,\n",
        "    feedforward_dim = 128,\n",
        "    do_pos_embed = True,\n",
        "    pos_embed_dim = 16,\n",
        "    dim_blocks = [16, 32, 32, 32],\n",
        "    res_blocks = [True, True, True, True],\n",
        "    attn_blocks = [False, False, True, True],\n",
        "    num_heads = 8,\n",
        "    kernel_size = 3,\n",
        "    activation = nn.GELU,\n",
        "    dropout = 0.0,\n",
        "    log_wandb = True,\n",
        "\n",
        "    # Pretraining\n",
        "    criterion = nn.CrossEntropyLoss,\n",
        "    optimizer = optim.Adam,\n",
        "    lr = 0.001,\n",
        "    num_epochs = 30,\n",
        "    batch_size = 16,\n",
        "    log_freq = 50,\n",
        "    val_batches = 50,\n",
        "    use_weight = True,\n",
        "\n",
        "    # Finetuning\n",
        "    num_epochs_ft = 100,\n",
        "    ft_last_block = False,\n",
        "    log_freq_ft = 10\n",
        ")\n",
        "try:\n",
        "    os.mkdir(params.pretrain_dir)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir(params.finetune_dir)\n",
        "except FileExistsError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9575d1f",
      "metadata": {
        "id": "f9575d1f"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accusleep Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg = np.load(\"data/4-hour_recordings/Mouse01/Day1/EEG_preproc.npy\")\n",
        "emg = np.load(\"data/4-hour_recordings/Mouse01/Day1/EMG_preproc.npy\")\n",
        "i = 100\n",
        "window_size = 5\n",
        "window = slice(128*i, int(128*(i + window_size)))\n",
        "plt.plot(eeg[window])\n",
        "plt.title(\"EEG\")\n",
        "plt.show()\n",
        "plt.plot(emg[window])\n",
        "plt.title(\"EMG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Homemade Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_emg = np.load(\"data/homemade_recordings/EEG-EMG_preproc.npy\")\n",
        "i = 20\n",
        "window_size = 5\n",
        "window = slice(128*i, int(128*(i + window_size)))\n",
        "plt.plot(eeg_emg[params.homemade_eeg_idx, window])\n",
        "plt.title(\"EEG\")\n",
        "plt.show()\n",
        "plt.plot(eeg_emg[params.homemade_emg_idx, window])\n",
        "plt.title(\"EMG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd7def18",
      "metadata": {
        "id": "dd7def18"
      },
      "outputs": [],
      "source": [
        "sample_data = SleepDataset(\n",
        "    params.sample_data_dir,\n",
        "    params.accusleep_sf,\n",
        "    params.model_sf,\n",
        "    params.accusleep_epoch_len,\n",
        "    params.model_epoch_len,\n",
        "    False,\n",
        "    run_preproc=params.run_preproc,\n",
        "    bandpass_freqs=params.bandpass_freqs,\n",
        "    context_window=params.context_window,\n",
        "    random_shift=params.random_shift,\n",
        "    eeg_transform=params.eeg_transform,\n",
        "    emg_transform=params.emg_transform,\n",
        "    target_transform=params.target_transform\n",
        ")\n",
        "sample_loader = DataLoader(\n",
        "    sample_data,\n",
        "    params.batch_size,\n",
        "    shuffle=False,\n",
        "    pin_memory=False\n",
        ")\n",
        "params.input_shape = next(iter(sample_loader))[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159da10b",
      "metadata": {
        "id": "159da10b"
      },
      "outputs": [],
      "source": [
        "def get_pretrain_dataloaders(params):\n",
        "    # Datasets\n",
        "    train_data = SleepDataset(\n",
        "        params.train_data_dir,\n",
        "        params.accusleep_sf,\n",
        "        params.model_sf,\n",
        "        params.accusleep_epoch_len,\n",
        "        params.model_epoch_len,\n",
        "        False,\n",
        "        run_preproc=params.run_preproc,\n",
        "        bandpass_freqs=params.bandpass_freqs,\n",
        "        context_window=params.context_window,\n",
        "        random_shift=params.random_shift,\n",
        "        eeg_transform=params.eeg_transform,\n",
        "        emg_transform=params.emg_transform,\n",
        "        target_transform=params.target_transform\n",
        "    )\n",
        "    test_data = SleepDataset(\n",
        "        params.sample_data_dir,\n",
        "        params.accusleep_sf,\n",
        "        params.model_sf,\n",
        "        params.accusleep_epoch_len,\n",
        "        params.model_epoch_len,\n",
        "        False,\n",
        "        run_preproc=params.run_preproc,\n",
        "        bandpass_freqs=params.bandpass_freqs,\n",
        "        context_window=params.context_window,\n",
        "        random_shift=params.random_shift,\n",
        "        eeg_transform=params.eeg_transform,\n",
        "        emg_transform=params.emg_transform,\n",
        "        target_transform=params.target_transform\n",
        "    )\n",
        "    homemade_test_data = SleepDataset(\n",
        "        params.homemade_data_dir,\n",
        "        params.homemade_sf,\n",
        "        params.model_sf,\n",
        "        params.homemade_epoch_len,\n",
        "        params.model_epoch_len,\n",
        "        True,\n",
        "        run_preproc=params.run_preproc,\n",
        "        bandpass_freqs=params.bandpass_freqs,\n",
        "        context_window=params.context_window,\n",
        "        eeg_idx=9,\n",
        "        emg_idx=10,\n",
        "        random_shift=params.random_shift,\n",
        "        eeg_transform=params.eeg_transform,\n",
        "        emg_transform=params.emg_transform,\n",
        "        target_transform=params.target_transform\n",
        "    )\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        params.batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        num_workers=params.threads_per_gpu\n",
        "    )\n",
        "    indices = np.random.choice(len(test_data), params.val_batches*params.batch_size, replace=False)\n",
        "    val_sampler = SubsetRandomSampler(indices)\n",
        "    val_loader = DataLoader(\n",
        "        test_data,\n",
        "        params.batch_size,\n",
        "        sampler=val_sampler,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        num_workers=params.threads_per_gpu\n",
        "    )\n",
        "    homemade_test_loader = DataLoader(\n",
        "        homemade_test_data,\n",
        "        params.batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        num_workers=params.threads_per_gpu\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, homemade_test_loader, train_data.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38eb21d7",
      "metadata": {
        "id": "38eb21d7"
      },
      "outputs": [],
      "source": [
        "def get_finetune_dataloaders(params):\n",
        "    # Datasets\n",
        "    homemade_train_data = SleepDataset(\n",
        "        params.homemade_data_dir,\n",
        "        params.homemade_sf,\n",
        "        params.model_sf,\n",
        "        params.homemade_epoch_len,\n",
        "        params.model_epoch_len,\n",
        "        True,\n",
        "        run_preproc=params.run_preproc,\n",
        "        bandpass_freqs=params.bandpass_freqs,\n",
        "        context_window=params.context_window,\n",
        "        eeg_idx=params.homemade_eeg_idx,\n",
        "        emg_idx=params.homemade_emg_idx,\n",
        "        random_shift=params.random_shift,\n",
        "        eeg_transform=params.eeg_transform,\n",
        "        emg_transform=params.emg_transform,\n",
        "        target_transform=params.target_transform\n",
        "    )\n",
        "\n",
        "    homemade_test_data = SleepDataset(\n",
        "        params.homemade_data_dir,\n",
        "        params.homemade_sf,\n",
        "        params.model_sf,\n",
        "        params.homemade_epoch_len,\n",
        "        params.model_epoch_len,\n",
        "        True,\n",
        "        run_preproc=params.run_preproc,\n",
        "        bandpass_freqs=params.bandpass_freqs,\n",
        "        context_window=params.context_window,\n",
        "        eeg_idx=9,\n",
        "        emg_idx=10,\n",
        "        random_shift=params.random_shift,\n",
        "        eeg_transform=params.eeg_transform,\n",
        "        emg_transform=params.emg_transform,\n",
        "        target_transform=params.target_transform\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    homemade_train_loader = DataLoader(\n",
        "        homemade_train_data,\n",
        "        params.batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        num_workers=params.threads_per_gpu\n",
        "    )\n",
        "    homemade_test_loader = DataLoader(\n",
        "        homemade_test_data,\n",
        "        params.batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        num_workers=params.threads_per_gpu\n",
        "    )\n",
        "\n",
        "    return homemade_train_loader, homemade_test_loader, homemade_train_data.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61cec7e8",
      "metadata": {
        "id": "61cec7e8"
      },
      "source": [
        "## 1D Convolutional Attention Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e9e266",
      "metadata": {
        "id": "d5e9e266"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, activation):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=\"same\")\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=\"same\")\n",
        "        self.activation = activation()\n",
        "        self.downsample = (\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "            if in_channels != out_channels\n",
        "            else nn.Identity()\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(in_channels)\n",
        "        self.norm2 = nn.LayerNorm(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.downsample(x)\n",
        "        x = self.norm1(x.transpose(1, 2)).transpose(1, 2)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm2(x.transpose(1, 2)).transpose(1, 2)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + res\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, do_ln, activation):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(in_dim) if do_ln else nn.Identity()\n",
        "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.activation = activation()\n",
        "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "    original_depth = depth\n",
        "    if depth % 2 != 0:\n",
        "        depth += 1\n",
        "    depth /= 2\n",
        "\n",
        "    positions = np.arange(length)[:, np.newaxis]  # (length, 1)\n",
        "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
        "\n",
        "    angle_rads = positions / (10000**depths)\n",
        "\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)[\n",
        "        :, :original_depth\n",
        "    ]\n",
        "\n",
        "    return torch.Tensor(pos_encoding)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, seq_length, out_dim, ff_dim, activation):\n",
        "        super().__init__()\n",
        "        pos_encoding = positional_encoding(seq_length, out_dim)[None, :, :]\n",
        "        self.pos_encoding = nn.Parameter(pos_encoding, requires_grad=False)\n",
        "        self.ffn = FeedForward(out_dim, ff_dim, out_dim, False, activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1,2)\n",
        "        x += self.ffn(self.pos_encoding)\n",
        "        return x.transpose(1,2)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_dim, embed_dim, num_heads, feedforward_dim, activation, dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.qkv = nn.Linear(in_dim, embed_dim * 3)\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.feed_forward = FeedForward(\n",
        "            in_dim, feedforward_dim, in_dim, True, activation\n",
        "        )\n",
        "        self.proj = (\n",
        "            nn.Linear(embed_dim, in_dim) if in_dim != embed_dim else nn.Identity()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(in_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        res = x\n",
        "        x = self.norm1(x)\n",
        "        q, k, v = self.qkv(x).chunk(3, -1)\n",
        "        x, _ = self.attention(q, k, v, need_weights=False)\n",
        "        x = self.proj(x)\n",
        "        x += res\n",
        "        res = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x += res\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape,\n",
        "        in_dim,\n",
        "        out_dim,\n",
        "        embed_dim,\n",
        "        feedforward_dim,\n",
        "        do_pos_embed,\n",
        "        pos_embed_dim,\n",
        "        dim_blocks,\n",
        "        res_blocks,\n",
        "        attn_blocks,\n",
        "        num_heads,\n",
        "        kernel_size,\n",
        "        activation,\n",
        "        dropout,\n",
        "        **params\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert len(dim_blocks) == len(attn_blocks) == len(res_blocks)\n",
        "        seq_length = input_shape[-1]\n",
        "        self.num_blocks = len(dim_blocks)\n",
        "        res_blocks = (\n",
        "            res_blocks\n",
        "            if isinstance(res_blocks, list)\n",
        "            else [res_blocks] * self.num_blocks\n",
        "        )\n",
        "        attn_blocks = (\n",
        "            attn_blocks\n",
        "            if isinstance(attn_blocks, list)\n",
        "            else [attn_blocks] * self.num_blocks\n",
        "        )\n",
        "\n",
        "        self.pos_embed = (\n",
        "            PositionalEmbedding(seq_length, in_dim, pos_embed_dim, activation)\n",
        "            if do_pos_embed\n",
        "            else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i, (dim_block, do_res, do_attn) in enumerate(\n",
        "            zip(dim_blocks, res_blocks, attn_blocks)\n",
        "        ):\n",
        "            assert do_res or do_attn\n",
        "            block = nn.Module()\n",
        "            block.res = (\n",
        "                ResidualBlock(\n",
        "                    in_dim if i == 0 else dim_blocks[i - 1],\n",
        "                    dim_block,\n",
        "                    kernel_size,\n",
        "                    activation,\n",
        "                )\n",
        "                if do_res\n",
        "                else nn.Identity()\n",
        "            )\n",
        "            block.attn = (\n",
        "                AttentionBlock(\n",
        "                    dim_block,\n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    feedforward_dim,\n",
        "                    activation,\n",
        "                    dropout,\n",
        "                )\n",
        "                if do_attn\n",
        "                else nn.Identity()\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        self.out_block = nn.Sequential(\n",
        "            nn.Linear(seq_length * dim_blocks[-1], 64),\n",
        "            activation(),\n",
        "            nn.Linear(64, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pos_embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block.res(x)\n",
        "            x = block.attn(x)\n",
        "        return self.out_block(x.flatten(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c82e5d32",
      "metadata": {
        "id": "c82e5d32"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e7967b3",
      "metadata": {
        "id": "9e7967b3"
      },
      "outputs": [],
      "source": [
        "def pretrain(params):\n",
        "    torch.set_num_threads(1)\n",
        "    project_config = ProjectConfiguration(project_dir=os.path.join(os.getcwd(), params.pretrain_dir), automatic_checkpoint_naming=True)\n",
        "    accelerator = Accelerator(log_with=\"wandb\", project_config=project_config)\n",
        "\n",
        "    if params.log_wandb:\n",
        "        accelerator.init_trackers(\n",
        "            project_name=\"Sleep Staging\",\n",
        "            config=params\n",
        "        )\n",
        "\n",
        "    model = Model(**params)\n",
        "    optimizer = params.optimizer(model.parameters(), params.lr)\n",
        "    train_loader, val_loader, homemade_test_loader, weight = get_pretrain_dataloaders(params)\n",
        "    criterion = params.criterion(weight=weight.to(accelerator.device) if params.use_weight else None)\n",
        "\n",
        "    train_loader, val_loader, homemade_test_loader, model, optimizer = accelerator.prepare(\n",
        "        train_loader, val_loader, homemade_test_loader, model, optimizer)\n",
        "\n",
        "    for epoch in range(params.num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        avg_loss = 0\n",
        "        avg_acc = 0\n",
        "        if params.log_wandb:\n",
        "            accelerator.log(dict(epoch = epoch))\n",
        "\n",
        "        bar = tqdm(train_loader, desc=(\n",
        "            f\"Training | Epoch: {epoch} | \"\n",
        "            f\"Acc: {avg_acc:.2%} | \"\n",
        "            f\"Loss: {avg_loss:.4f}\"),\n",
        "            disable=not accelerator.is_main_process\n",
        "        )\n",
        "        for i, (x, y) in enumerate(bar):\n",
        "            # Forward pass\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            total_loss += loss\n",
        "            avg_loss += loss\n",
        "\n",
        "            # Calculate Accuracy\n",
        "            acc = (pred.argmax(-1) == y).float().mean()\n",
        "            total_acc += acc\n",
        "            avg_acc += acc\n",
        "\n",
        "            # Log\n",
        "            if (i + 1) % params.log_freq == 0:\n",
        "                avg_loss /= params.log_freq\n",
        "                avg_acc /= params.log_freq\n",
        "                avg_loss = accelerator.gather(avg_loss).mean()\n",
        "                avg_acc = accelerator.gather(avg_acc).mean()\n",
        "                if params.log_wandb:\n",
        "                    accelerator.log(dict(\n",
        "                        train_loss = avg_loss,\n",
        "                        train_acc = avg_acc)\n",
        "                    )\n",
        "                bar.set_description(f\"Training | Epoch: {epoch} | \"\n",
        "                                f\"Acc: {avg_acc:.2%} | \"\n",
        "                                f\"Loss: {avg_loss:.4f}\")\n",
        "                avg_loss = 0\n",
        "                avg_acc = 0\n",
        "\n",
        "        if params.log_wandb:\n",
        "            train_epoch_loss = total_loss/len(train_loader)\n",
        "            train_epoch_acc = total_acc/len(train_loader)\n",
        "            train_epoch_loss = accelerator.gather(train_epoch_loss).mean()\n",
        "            train_epoch_acc = accelerator.gather(train_epoch_acc).mean()\n",
        "            accelerator.log(dict(\n",
        "                train_epoch_loss = train_epoch_loss,\n",
        "                train_epoch_acc = train_epoch_loss)\n",
        "            )\n",
        "\n",
        "        # Test Phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            for i, (x, y) in enumerate(val_loader):\n",
        "                # Forward pass\n",
        "                pred = model(x)\n",
        "                loss = criterion(pred, y)\n",
        "                total_loss += loss\n",
        "\n",
        "                # Calculate Accuracy\n",
        "                acc = (pred.argmax(-1) == y).float().mean()\n",
        "                total_acc += acc\n",
        "            if params.log_wandb:\n",
        "                val_epoch_loss = total_loss/len(val_loader)\n",
        "                val_epoch_acc = total_acc/len(val_loader)\n",
        "                val_epoch_loss = accelerator.gather(val_epoch_loss).mean()\n",
        "                val_epoch_acc = accelerator.gather(val_epoch_acc).mean()\n",
        "                accelerator.log(dict(\n",
        "                    val_epoch_loss = val_epoch_loss,\n",
        "                    val_epoch_acc = val_epoch_acc)\n",
        "                )\n",
        "\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            for i, (x, y) in enumerate(homemade_test_loader):\n",
        "                # Forward pass\n",
        "                pred = model(x)\n",
        "                loss = criterion(pred, y)\n",
        "                total_loss += loss\n",
        "\n",
        "                # Calculate Accuracy\n",
        "                acc = (pred.argmax(-1) == y).float().mean()\n",
        "                total_acc += acc\n",
        "            if params.log_wandb:\n",
        "                homemade_epoch_loss = total_loss/len(homemade_test_loader)\n",
        "                homemade_epoch_acc = total_acc/len(homemade_test_loader)\n",
        "                homemade_epoch_loss = accelerator.gather(homemade_epoch_loss).mean()\n",
        "                homemade_epoch_acc = accelerator.gather(homemade_epoch_acc).mean()\n",
        "                accelerator.log(dict(\n",
        "                    homemade_epoch_loss = total_loss,\n",
        "                    homemade_epoch_acc = total_acc)\n",
        "                )\n",
        "\n",
        "        # Save Model\n",
        "        accelerator.save_state()\n",
        "\n",
        "    accelerator.end_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e37649",
      "metadata": {
        "id": "d6e37649",
        "outputId": "f8669a95-57f1-487c-dff3-b213062670fe"
      },
      "outputs": [],
      "source": [
        "accelerate.notebook_launcher(pretrain, (params,), num_processes=params.gpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e1f6dd",
      "metadata": {
        "id": "63e1f6dd"
      },
      "source": [
        "## Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75640eda",
      "metadata": {
        "id": "75640eda"
      },
      "outputs": [],
      "source": [
        "def finetune(params, checkpoint_idx):\n",
        "    torch.set_num_threads(1)\n",
        "    project_config = ProjectConfiguration(project_dir=os.path.join(os.getcwd(), params.finetune_dir), automatic_checkpoint_naming=True)\n",
        "    accelerator = Accelerator(log_with=\"wandb\", project_config=project_config)\n",
        "\n",
        "    if params.log_wandb:\n",
        "        accelerator.init_trackers(\n",
        "            project_name=\"Sleep Staging\",\n",
        "            config=params\n",
        "        )\n",
        "\n",
        "    model = Model(**params)\n",
        "    state_dict = torch.load(os.path.join(os.getcwd(), params.pretrain_dir, f\"checkpoints/checkpoint_{checkpoint_idx}/pytorch_model.bin\"))\n",
        "    model.load_state_dict(state_dict)\n",
        "    if params.ft_last_block:\n",
        "        for name, param in model.named_parameters():\n",
        "            param.requires_grad = True if \"out_block\" in name else False\n",
        "\n",
        "    optimizer = params.optimizer(model.parameters(), params.lr)\n",
        "\n",
        "    homemade_train_loader, homemade_test_loader, weight = get_finetune_dataloaders(params)\n",
        "    criterion = params.criterion(weight=weight.to(accelerator.device) if params.use_weight else None)\n",
        "    \n",
        "    homemade_train_loader, homemade_test_loader, model, optimizer = accelerator.prepare(\n",
        "        homemade_train_loader, homemade_test_loader, model, optimizer)\n",
        "\n",
        "    for epoch in range(params.num_epochs_ft):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        avg_loss = 0\n",
        "        avg_acc = 0\n",
        "        if params.log_wandb:\n",
        "            accelerator.log(dict(epoch_ft = epoch))\n",
        "        bar = tqdm(homemade_train_loader, desc=(\n",
        "            f\"Training | Epoch: {epoch} | \"\n",
        "            f\"Acc: {avg_acc:.2%} | \"\n",
        "            f\"Loss: {avg_loss:.4f}\"),\n",
        "            disable=not accelerator.is_main_process\n",
        "        )\n",
        "        for i, (x, y) in enumerate(bar):\n",
        "            # Forward pass\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            total_loss += loss\n",
        "            avg_loss += loss\n",
        "\n",
        "            # Calculate Accuracy\n",
        "            acc = (pred.argmax(-1) == y).float().mean()\n",
        "            total_acc += acc\n",
        "            avg_acc += acc\n",
        "\n",
        "            # Log\n",
        "            if (i + 1) % params.log_freq_ft == 0:\n",
        "                avg_loss /= params.log_freq\n",
        "                avg_acc /= params.log_freq\n",
        "                avg_loss = accelerator.gather(avg_loss).mean()\n",
        "                avg_acc = accelerator.gather(avg_acc).mean()\n",
        "                if params.log_wandb:\n",
        "                    accelerator.log(dict(\n",
        "                        homemade_train_loss = avg_loss,\n",
        "                        homemade_train_acc = avg_acc)\n",
        "                    )\n",
        "                bar.set_description(f\"Training | Epoch: {epoch} | \"\n",
        "                                f\"Acc: {avg_acc:.2%} | \"\n",
        "                                f\"Loss: {avg_loss:.4f}\")\n",
        "                avg_loss = 0\n",
        "                avg_acc = 0\n",
        "\n",
        "        if params.log_wandb:\n",
        "            train_epoch_loss = total_loss/len(homemade_train_loader)\n",
        "            train_epoch_acc = total_acc/len(homemade_train_loader)\n",
        "            train_epoch_loss = accelerator.gather(train_epoch_loss).mean()\n",
        "            train_epoch_acc = accelerator.gather(train_epoch_acc).mean()\n",
        "            accelerator.log(dict(\n",
        "                train_epoch_loss = train_epoch_loss,\n",
        "                train_epoch_acc = train_epoch_loss)\n",
        "            )\n",
        "\n",
        "        # Test Phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            for i, (x, y) in enumerate(homemade_test_loader):\n",
        "                # Forward pass\n",
        "                pred = model(x)\n",
        "                loss = criterion(pred, y)\n",
        "                total_loss += loss\n",
        "\n",
        "                # Calculate Accuracy\n",
        "                acc = (pred.argmax(-1) == y).float().mean()\n",
        "                total_acc += acc\n",
        "            if params.log_wandb:\n",
        "                homemade_epoch_loss = total_loss/len(homemade_test_loader)\n",
        "                homemade_epoch_acc = total_acc/len(homemade_test_loader)\n",
        "                homemade_epoch_loss = accelerator.gather(homemade_epoch_loss).mean()\n",
        "                homemade_epoch_acc = accelerator.gather(homemade_epoch_acc).mean()\n",
        "                accelerator.log(dict(\n",
        "                    homemade_epoch_loss = total_loss,\n",
        "                    homemade_epoch_acc = total_acc)\n",
        "                )\n",
        "\n",
        "        # Save Model\n",
        "        accelerator.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I1FSrwWrWIZX",
      "metadata": {
        "id": "I1FSrwWrWIZX"
      },
      "outputs": [],
      "source": [
        "checkpoint_idx = 7\n",
        "accelerate.notebook_launcher(finetune, (params, checkpoint_idx), num_processes=params.gpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8afe992",
      "metadata": {
        "id": "c8afe992"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c996d0",
      "metadata": {
        "id": "79c996d0"
      },
      "outputs": [],
      "source": [
        "checkpoint_idx = 2\n",
        "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = Model(**params).to(device)\n",
        "state_dict = torch.load(os.path.join(os.getcwd(), params.finetune_dir, f\"checkpoints/checkpoint_{checkpoint_idx}/pytorch_model.bin\"))\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1195a3fb",
      "metadata": {
        "id": "1195a3fb"
      },
      "outputs": [],
      "source": [
        "# Homemade Test Phase\n",
        "model.eval()\n",
        "_, loader, weight = get_finetune_dataloaders(params)\n",
        "total_loss = 0\n",
        "total_acc = 0\n",
        "criterion = params.criterion(weight=weight.to(device))\n",
        "ys = []\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    t0 = time()\n",
        "    for i, (x, y) in enumerate(tqdm(loader, desc=\"Testing\")):\n",
        "        # Save ground truth for plotting\n",
        "        ys.append(y)\n",
        "\n",
        "        # Send to GPU (if available)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        pred = model(x)\n",
        "\n",
        "        loss = criterion(pred, y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate Accuracy\n",
        "        pred = pred.argmax(-1)\n",
        "        acc = (pred == y).float().mean()\n",
        "        total_acc += acc\n",
        "\n",
        "        # Save prediction for plotting\n",
        "        preds.append(pred.cpu())\n",
        "    t1 = time()\n",
        "    preds = torch.cat(preds)\n",
        "    ys = torch.cat(ys)\n",
        "\n",
        "print(f\"Loss: {total_loss/len(loader):4f}\\n\"\n",
        "      f\"Acc: {total_acc/len(loader):4f}\\n\"\n",
        "      f\"Time per pred: {(t1-t0)/len(loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(ys, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(confusion_matrix(ys, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d7989c",
      "metadata": {
        "id": "61d7989c"
      },
      "outputs": [],
      "source": [
        "window = slice(0, 500)\n",
        "_, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
        "ax1.plot(ys[window])\n",
        "ax1.set_title(\"Ground Truth\")\n",
        "ax2.plot(preds[window])\n",
        "ax2.set_title(\"Predicted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2636c61f",
      "metadata": {
        "id": "2636c61f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
