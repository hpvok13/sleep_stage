{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da745c8",
   "metadata": {},
   "source": [
    "## Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e41b9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: opencv-python in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.8.0.76)\n",
      "Requirement already satisfied: torch in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.15.2)\n",
      "Requirement already satisfied: torchaudio in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: torchsummary in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: scipy in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: pandas in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (2.0.3)\n",
      "Requirement already satisfied: matplotlib in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (3.7.2)\n",
      "Requirement already satisfied: mne in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: wandb in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.15.9)\n",
      "Requirement already satisfied: filelock in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torch->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 3)) (68.0.0)\n",
      "Requirement already satisfied: wheel in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 3)) (0.38.4)\n",
      "Requirement already satisfied: cmake in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 3)) (3.27.2)\n",
      "Requirement already satisfied: lit in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 3)) (16.0.6)\n",
      "Requirement already satisfied: requests in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torchvision->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from torchvision->-r requirements.txt (line 4)) (10.0.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 9)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 9)) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 9)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 10)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 10)) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 10)) (23.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 10)) (3.0.9)\n",
      "Requirement already satisfied: tqdm in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from mne->-r requirements.txt (line 11)) (4.66.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from mne->-r requirements.txt (line 11)) (1.7.0)\n",
      "Requirement already satisfied: decorator in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from mne->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (3.1.32)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (1.30.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (6.0)\n",
      "Requirement already satisfied: pathtools in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 12)) (4.24.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12)) (4.0.10)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from pooch>=1.5->mne->-r requirements.txt (line 11)) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision->-r requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision->-r requirements.txt (line 4)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12)) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c7cdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node081\n"
     ]
    }
   ],
   "source": [
    "# Check correct hostname\n",
    "!hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebce31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c879fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import resample\n",
    "from mne.filter import filter_data\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9fda2",
   "metadata": {},
   "source": [
    "## Helper Functions/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f65aaadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "#     nyq = 0.5 * fs\n",
    "#     low = lowcut / nyq\n",
    "#     high = highcut / nyq\n",
    "#     b, a = butter(order, [low, high], btype='band')\n",
    "#     return b, a\n",
    "\n",
    "\n",
    "# def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "#     b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     y = lfilter(b, a, data)\n",
    "#     return y\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    '''\n",
    "    Set up distributed data training.\n",
    "\n",
    "    Args:\n",
    "        rank (int): Unique identifier for each gpu\n",
    "        world_size (int): Total number of processes\n",
    "    '''\n",
    "\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355' # This is just a random port\n",
    "    \n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    cudnn.benchmark = True\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def ddp_cleanup():\n",
    "    destroy_process_group()\n",
    "\n",
    "\n",
    "def downsample(x, sf, new_sf):\n",
    "    num = int(new_sf / sf * x.shape[-1])\n",
    "    return resample(x, num, axis=-1)\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "class SleepDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        raw_sf,\n",
    "        model_sf,\n",
    "        raw_epoch_len,\n",
    "        model_epoch_len,\n",
    "        homemade,\n",
    "        *,\n",
    "        run_preproc=False,\n",
    "        bandpass_freqs=None,\n",
    "        context_window=1,\n",
    "        eeg_idx=0,\n",
    "        emg_idx=5,\n",
    "        random_shift=False,\n",
    "        eeg_transform=None,\n",
    "        emg_transform=None,\n",
    "        target_transform=None\n",
    "    ):\n",
    "        # Build the paths\n",
    "        all_paths = list(Path(data_dir).glob(\"**/*\"))\n",
    "        all_dirs = sorted([folder for folder in all_paths if folder.is_dir()])\n",
    "        if not all_dirs:\n",
    "            all_dirs = [Path(data_dir)]\n",
    "        # self.sf = sf\n",
    "        # self.epoch_len = epoch_len\n",
    "        assert (\n",
    "            float(raw_sf * raw_epoch_len).is_integer()\n",
    "            and float(model_sf * model_epoch_len).is_integer()\n",
    "        )\n",
    "        self.window_size = int(model_sf * model_epoch_len)\n",
    "        self.raw_sf = raw_sf\n",
    "        self.model_sf = model_sf\n",
    "        self.raw_epoch_len = raw_epoch_len\n",
    "        self.model_epoch_len = model_epoch_len\n",
    "        assert context_window >= 1\n",
    "        self.context_window = context_window\n",
    "        self.random_shift = random_shift\n",
    "        self.eeg_transform = eeg_transform\n",
    "        self.emg_transform = emg_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.eegs = []\n",
    "        self.emgs = []\n",
    "        self.labels = []\n",
    "        index_map = []\n",
    "        dir_map = []\n",
    "        i_map = 0\n",
    "        for dir in tqdm(all_dirs, desc=\"Loading Data\"):\n",
    "            try:\n",
    "                if run_preproc:\n",
    "                    self.preprocess(dir, homemade, raw_sf, model_sf, bandpass_freqs)\n",
    "                eeg, emg, label = self.get_preproc_files(\n",
    "                    dir, homemade, eeg_idx, emg_idx\n",
    "                )\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            self.eegs.append(eeg)\n",
    "            self.emgs.append(emg)\n",
    "            self.labels.append(label)\n",
    "            dir_map.append(np.repeat(i_map, len(label) - self.context_window + 1))\n",
    "            index_map.append(np.arange(self.context_window - 1, len(label)))\n",
    "            i_map += 1\n",
    "        if len(dir_map) == 0:\n",
    "            raise FileNotFoundError\n",
    "\n",
    "        dir_map = np.concatenate(dir_map)\n",
    "        index_map = np.concatenate(index_map)\n",
    "        self.index_map = np.column_stack((dir_map, index_map))\n",
    "\n",
    "    def get_preproc_files(self, data_dir, homemade, eeg_idx, emg_idx):\n",
    "        \"\"\"Retrieves EEG, EMG, and label data\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): directory in which data is stored\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: raised if EEG.mat, EMG.mat, or labels.mat do not exist within target_folder\n",
    "\n",
    "        Returns:\n",
    "            eeg_array, emg_array, label_array (torch.Tensor, torch.Tensor, torch.Tensor): EEG, EMG, and label data\n",
    "            as numpy arrays\n",
    "        \"\"\"\n",
    "        if homemade:\n",
    "            eeg_emg_file = os.path.join(data_dir, \"EEG-EMG_preproc.npy\")\n",
    "            label_file = os.path.join(data_dir, \"labels_preproc.npy\")\n",
    "\n",
    "            paths_are_files = list(\n",
    "                map(lambda x: os.path.isfile(x), [eeg_emg_file, label_file])\n",
    "            )\n",
    "            if not all(paths_are_files):\n",
    "                raise FileNotFoundError\n",
    "            # load files\n",
    "            eeg_emg = np.load(eeg_emg_file)\n",
    "            label = np.load(label_file)\n",
    "\n",
    "            eeg, emg = eeg_emg[eeg_idx], eeg_emg[emg_idx]\n",
    "        else:\n",
    "            eeg_file = os.path.join(data_dir, \"EEG_preproc.npy\")\n",
    "            emg_file = os.path.join(data_dir, \"EMG_preproc.npy\")\n",
    "            label_file = os.path.join(data_dir, \"labels_preproc.npy\")\n",
    "\n",
    "            paths_are_files = list(\n",
    "                map(lambda x: os.path.isfile(x), [eeg_file, emg_file, label_file])\n",
    "            )\n",
    "            if not all(paths_are_files):\n",
    "                raise FileNotFoundError\n",
    "            # load files\n",
    "            eeg = np.load(eeg_file)\n",
    "            emg = np.load(emg_file)\n",
    "            label = np.load(label_file)\n",
    "\n",
    "        # Check lengths\n",
    "        assert eeg.shape[0] == emg.shape[0]\n",
    "        num_eeg_samples = eeg.shape[0] / self.window_size\n",
    "        assert num_eeg_samples >= len(label)\n",
    "\n",
    "        eeg = eeg[: len(label) * self.window_size]\n",
    "        emg = emg[: len(label) * self.window_size]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(eeg, dtype=torch.float),\n",
    "            torch.tensor(emg, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        data_dir,\n",
    "        homemade,\n",
    "        raw_sf,\n",
    "        model_sf,\n",
    "        bandpass_freqs,\n",
    "    ):\n",
    "        if homemade:\n",
    "            eeg_emg_file = os.path.join(data_dir, \"EEG-EMG.npy\")\n",
    "            label_file = os.path.join(data_dir, \"labels.npy\")\n",
    "\n",
    "            paths_are_files = list(\n",
    "                map(lambda x: os.path.isfile(x), [eeg_emg_file, label_file])\n",
    "            )\n",
    "            if not all(paths_are_files):\n",
    "                raise FileNotFoundError\n",
    "            # load files\n",
    "            eeg_emg = np.load(eeg_emg_file)\n",
    "            label = np.load(label_file)\n",
    "\n",
    "            if bandpass_freqs:\n",
    "                eeg_emg = filter_data(\n",
    "                    eeg_emg.astype(\"float64\"),\n",
    "                    raw_sf,\n",
    "                    bandpass_freqs[0],\n",
    "                    bandpass_freqs[1],\n",
    "                    verbose=0,\n",
    "                )\n",
    "            if raw_sf != model_sf:\n",
    "                eeg_emg = downsample(eeg_emg, raw_sf, model_sf)\n",
    "\n",
    "            # Scale EEG and EMG\n",
    "            eeg_emg = RobustScaler().fit_transform(eeg_emg.T).T  # type: ignore\n",
    "\n",
    "            num_samples = int(eeg_emg.shape[1] // (model_sf * self.raw_epoch_len))\n",
    "            assert num_samples == len(label)\n",
    "\n",
    "            if self.raw_epoch_len != self.model_epoch_len:\n",
    "                label_new_len = int(\n",
    "                    len(label) * self.raw_epoch_len // self.model_epoch_len\n",
    "                )\n",
    "                eeg_emg = eeg_emg[\n",
    "                    : int(label_new_len * model_sf * self.model_epoch_len)\n",
    "                ]\n",
    "\n",
    "            np.save(os.path.join(data_dir, \"EEG-EMG_preproc.npy\"), eeg_emg)\n",
    "        else:\n",
    "            eeg_file = os.path.join(data_dir, \"EEG.mat\")\n",
    "            emg_file = os.path.join(data_dir, \"EMG.mat\")\n",
    "            label_file = os.path.join(data_dir, \"labels.mat\")\n",
    "\n",
    "            paths_are_files = list(\n",
    "                map(lambda x: os.path.isfile(x), [eeg_file, emg_file, label_file])\n",
    "            )\n",
    "            if not all(paths_are_files):\n",
    "                raise FileNotFoundError\n",
    "            # load files\n",
    "            eeg = loadmat(eeg_file)\n",
    "            emg = loadmat(emg_file)\n",
    "            label = loadmat(label_file)\n",
    "\n",
    "            eeg = np.squeeze(eeg[\"EEG\"])\n",
    "            emg = np.squeeze(emg[\"EMG\"])\n",
    "            label = np.squeeze(label[\"labels\"])\n",
    "\n",
    "            assert eeg.shape[0] == emg.shape[0]\n",
    "\n",
    "            if bandpass_freqs:\n",
    "                eeg = filter_data(\n",
    "                    eeg.astype(\"float64\"),\n",
    "                    raw_sf,\n",
    "                    bandpass_freqs[0],\n",
    "                    bandpass_freqs[1],\n",
    "                    verbose=0,\n",
    "                )\n",
    "                emg = filter_data(\n",
    "                    emg.astype(\"float64\"),\n",
    "                    raw_sf,\n",
    "                    bandpass_freqs[0],\n",
    "                    bandpass_freqs[1],\n",
    "                    verbose=0,\n",
    "                )\n",
    "            if raw_sf != model_sf:\n",
    "                eeg = downsample(eeg, raw_sf, model_sf)\n",
    "                emg = downsample(emg, raw_sf, model_sf)\n",
    "\n",
    "            # Scale EEG and EMG\n",
    "            eeg = RobustScaler().fit_transform(eeg[:, np.newaxis]).squeeze()  # type: ignore\n",
    "            emg = RobustScaler().fit_transform(emg[:, np.newaxis]).squeeze()  # type: ignore\n",
    "            \n",
    "            num_samples = int(eeg.shape[0] // (model_sf * self.raw_epoch_len))\n",
    "            assert num_samples == len(label)\n",
    "\n",
    "            if self.raw_epoch_len != self.model_epoch_len:\n",
    "                label_new_len = int(\n",
    "                    len(label) * self.raw_epoch_len // self.model_epoch_len\n",
    "                )\n",
    "                eeg = eeg[: int(label_new_len * model_sf * self.model_epoch_len)]\n",
    "                emg = emg[: int(label_new_len * model_sf * self.model_epoch_len)]\n",
    "\n",
    "            np.save(os.path.join(data_dir, \"EEG_preproc.npy\"), eeg)\n",
    "            np.save(os.path.join(data_dir, \"EMG_preproc.npy\"), emg)\n",
    "\n",
    "        accusleep_dict = {\n",
    "            1: 2,  # \"R\",\n",
    "            2: 0,  # \"W\",\n",
    "            3: 1,  # \"N\"\n",
    "        }\n",
    "        # re-map the label values\n",
    "        label_df = pd.DataFrame({\"label\": label})\n",
    "        label_df[\"label\"] = label_df[\"label\"].map(accusleep_dict)\n",
    "        label = label_df[\"label\"].values\n",
    "\n",
    "        if self.raw_epoch_len != self.model_epoch_len:\n",
    "            label_new = np.zeros(label_new_len, dtype=int)  # type: ignore\n",
    "            for i in range(len(label_new)):\n",
    "                label_new[i] = label[\n",
    "                    int(round(i * self.model_epoch_len / self.raw_epoch_len))\n",
    "                ]\n",
    "            label = label_new\n",
    "\n",
    "        np.save(os.path.join(data_dir, \"labels_preproc.npy\"), label)  # type: ignore\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index_map.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dir_idx, adjusted_idx = self.index_map[idx]\n",
    "        label = self.labels[dir_idx][adjusted_idx]\n",
    "        if self.random_shift and adjusted_idx < len(self.labels[dir_idx]) - 1:\n",
    "            offset = np.random.randint(self.window_size)\n",
    "        else:\n",
    "            offset = 0\n",
    "        low = (adjusted_idx - self.context_window + 1) * self.window_size + offset\n",
    "        high = (adjusted_idx + 1) * self.window_size + offset\n",
    "        eeg = self.eegs[dir_idx][low:high]\n",
    "        emg = self.emgs[dir_idx][low:high]\n",
    "        if self.eeg_transform:\n",
    "            eeg = self.eeg_transform(eeg)\n",
    "        if self.emg_transform:\n",
    "            emg = self.emg_transform(emg)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        x = torch.stack((eeg, emg), dim=0)\n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a12c4",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b722a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = AttrDict(\n",
    "    # Data\n",
    "    train_data_dir = 'data/24-hour_recordings',\n",
    "    test_data_dir = 'data/4-hour_recordings',\n",
    "    sample_data_dir = 'data/4-hour_recordings/Mouse01/Day1',\n",
    "    homemade_data_dir = 'data/homemade_recordings',\n",
    "    pretrained_model_dir = 'models',\n",
    "    finetuned_model_dir = 'ft_models',\n",
    "    homemade_eeg_idx = 4,\n",
    "    homemade_emg_idx = 5,\n",
    "    model_sf = 128,\n",
    "    accusleep_sf = 512, # Hz\n",
    "    homemade_sf = 1000, # Hz\n",
    "    bandpass_freqs = [1, 64],\n",
    "    run_preproc = False,\n",
    "    homemade_epoch_len = 2.0, # sec\n",
    "    accusleep_epoch_len = 2.5, # sec\n",
    "    model_epoch_len = 2.5, # sec\n",
    "    context_window = 3,\n",
    "    random_shift = True,\n",
    "    eeg_transform = None,\n",
    "    emg_transform = None,\n",
    "    target_transform = None,\n",
    "    \n",
    "    # Model\n",
    "    in_dim = 2,\n",
    "    out_dim = 3,\n",
    "    embed_dim = 32,\n",
    "    feedforward_dim = 128,\n",
    "    do_pos_embed = True,\n",
    "    pos_embed_dim = 16,\n",
    "    dim_blocks = [16, 32, 32, 32],\n",
    "    res_blocks = [True, True, True, True],\n",
    "    attn_blocks = [False, False, True, True],\n",
    "    num_heads = 8,\n",
    "    kernel_size = 3,\n",
    "    activation = nn.GELU,\n",
    "    dropout = 0.0,\n",
    "    log_wandb = True,\n",
    "    \n",
    "    # Training\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    optimizer = optim.Adam,\n",
    "    lr = 0.001,\n",
    "    num_epochs = 30,\n",
    "    num_epochs_ft = 100,\n",
    "    batch_size = 16,\n",
    "    log_freq = 50,\n",
    "    val_batches=50\n",
    ")\n",
    "try:\n",
    "    os.mkdir(params.pretrained_model_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(params.finetuned_model_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "world_size = torch.cuda.device_count()\n",
    "do_ddp = world_size > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9575d1f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd7def18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 25/25 [00:04<00:00,  5.58it/s]\n",
      "Loading Data: 100%|██████████| 60/60 [00:03<00:00, 16.19it/s]\n",
      "Loading Data: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "Loading Data: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Loading Data: 100%|██████████| 1/1 [00:00<00:00, 43.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = SleepDataset(\n",
    "    params.train_data_dir,\n",
    "    params.accusleep_sf,\n",
    "    params.model_sf,\n",
    "    params.accusleep_epoch_len,\n",
    "    params.model_epoch_len,\n",
    "    False,\n",
    "    run_preproc=params.run_preproc,\n",
    "    bandpass_freqs=params.bandpass_freqs,\n",
    "    context_window=params.context_window,\n",
    "    random_shift=params.random_shift,\n",
    "    eeg_transform=params.eeg_transform,\n",
    "    emg_transform=params.emg_transform,\n",
    "    target_transform=params.target_transform\n",
    ")\n",
    "\n",
    "test_data = SleepDataset(\n",
    "    params.test_data_dir,\n",
    "    params.accusleep_sf,\n",
    "    params.model_sf,\n",
    "    params.accusleep_epoch_len,\n",
    "    params.model_epoch_len,\n",
    "    False,\n",
    "    run_preproc=params.run_preproc,\n",
    "    bandpass_freqs=params.bandpass_freqs,\n",
    "    context_window=params.context_window,\n",
    "    random_shift=params.random_shift,\n",
    "    eeg_transform=params.eeg_transform,\n",
    "    emg_transform=params.emg_transform,\n",
    "    target_transform=params.target_transform\n",
    ")\n",
    "\n",
    "homemade_train_data = SleepDataset(\n",
    "    params.homemade_data_dir,\n",
    "    params.homemade_sf,\n",
    "    params.model_sf,\n",
    "    params.homemade_epoch_len,\n",
    "    params.model_epoch_len,\n",
    "    True,\n",
    "    run_preproc=params.run_preproc,\n",
    "    bandpass_freqs=params.bandpass_freqs,\n",
    "    context_window=params.context_window,\n",
    "    eeg_idx=params.homemade_eeg_idx,\n",
    "    emg_idx=params.homemade_emg_idx,\n",
    "    random_shift=params.random_shift,\n",
    "    eeg_transform=params.eeg_transform,\n",
    "    emg_transform=params.emg_transform,\n",
    "    target_transform=params.target_transform\n",
    ")\n",
    "\n",
    "homemade_test_data = SleepDataset(\n",
    "    params.homemade_data_dir,\n",
    "    params.homemade_sf,\n",
    "    params.model_sf,\n",
    "    params.homemade_epoch_len,\n",
    "    params.model_epoch_len,\n",
    "    True,\n",
    "    run_preproc=params.run_preproc,\n",
    "    bandpass_freqs=params.bandpass_freqs,\n",
    "    context_window=params.context_window,\n",
    "    eeg_idx=9,\n",
    "    emg_idx=10,\n",
    "    random_shift=params.random_shift,\n",
    "    eeg_transform=params.eeg_transform,\n",
    "    emg_transform=params.emg_transform,\n",
    "    target_transform=params.target_transform\n",
    ")\n",
    "\n",
    "sample_data = SleepDataset(\n",
    "    params.sample_data_dir,\n",
    "    params.accusleep_sf,\n",
    "    params.model_sf,\n",
    "    params.accusleep_epoch_len,\n",
    "    params.model_epoch_len,\n",
    "    False,\n",
    "    run_preproc=params.run_preproc,\n",
    "    bandpass_freqs=params.bandpass_freqs,\n",
    "    context_window=params.context_window,\n",
    "    random_shift=params.random_shift,\n",
    "    eeg_transform=params.eeg_transform,\n",
    "    emg_transform=params.emg_transform,\n",
    "    target_transform=params.target_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc88c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    params.batch_size,\n",
    "    sampler=DistributedSampler(train_data) if do_ddp else None,\n",
    "    shuffle=not do_ddp,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    params.batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "indices = np.random.choice(len(test_data), params.val_batches*params.batch_size, replace=False)\n",
    "val_sampler = SubsetRandomSampler(indices)\n",
    "val_loader = DataLoader(\n",
    "    test_data,\n",
    "    params.batch_size,\n",
    "    sampler=val_sampler,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "sample_loader = DataLoader(\n",
    "    sample_data,\n",
    "    params.batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "homemade_train_loader = DataLoader(\n",
    "    homemade_train_data,\n",
    "    params.batch_size,\n",
    "    sampler=DistributedSampler(train_data) if do_ddp else None,\n",
    "    shuffle=not do_ddp,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "homemade_test_loader = DataLoader(\n",
    "    homemade_test_data,\n",
    "    params.batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "params.input_shape = next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cec7e8",
   "metadata": {},
   "source": [
    "## 1D Convolutional Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e9e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=\"same\")\n",
    "        self.activation = activation()\n",
    "        self.downsample = (\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(in_channels)\n",
    "        self.norm2 = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.downsample(x)\n",
    "        x = self.norm1(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm2(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + res\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, do_ln, activation):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_dim) if do_ln else nn.Identity()\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = activation()\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    original_depth = depth\n",
    "    if depth % 2 != 0:\n",
    "        depth += 1\n",
    "    depth /= 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (length, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rads = positions / (10000**depths)\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)[\n",
    "        :, :original_depth\n",
    "    ]\n",
    "\n",
    "    return torch.Tensor(pos_encoding)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, seq_length, out_dim, ff_dim, activation):\n",
    "        super().__init__()\n",
    "        pos_encoding = positional_encoding(seq_length, out_dim)[None, :, :]\n",
    "        self.pos_encoding = nn.Parameter(pos_encoding, requires_grad=False)\n",
    "        self.ffn = FeedForward(out_dim, ff_dim, out_dim, False, activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        x += self.ffn(self.pos_encoding)\n",
    "        return x.transpose(1,2)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim, embed_dim, num_heads, feedforward_dim, activation, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(in_dim, embed_dim * 3)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            in_dim, feedforward_dim, in_dim, True, activation\n",
    "        )\n",
    "        self.proj = (\n",
    "            nn.Linear(embed_dim, in_dim) if in_dim != embed_dim else nn.Identity()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        q, k, v = self.qkv(x).chunk(3, -1)\n",
    "        x, _ = self.attention(q, k, v, need_weights=False)\n",
    "        x = self.proj(x)\n",
    "        x += res\n",
    "        res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x += res\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        embed_dim,\n",
    "        feedforward_dim,\n",
    "        do_pos_embed,\n",
    "        pos_embed_dim,\n",
    "        dim_blocks,\n",
    "        res_blocks,\n",
    "        attn_blocks,\n",
    "        num_heads,\n",
    "        kernel_size,\n",
    "        activation,\n",
    "        dropout,\n",
    "        **params\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(dim_blocks) == len(attn_blocks) == len(res_blocks)\n",
    "        seq_length = input_shape[-1]\n",
    "        self.num_blocks = len(dim_blocks)\n",
    "        res_blocks = (\n",
    "            res_blocks\n",
    "            if isinstance(res_blocks, list)\n",
    "            else [res_blocks] * self.num_blocks\n",
    "        )\n",
    "        attn_blocks = (\n",
    "            attn_blocks\n",
    "            if isinstance(attn_blocks, list)\n",
    "            else [attn_blocks] * self.num_blocks\n",
    "        )\n",
    "\n",
    "        self.pos_embed = (\n",
    "            PositionalEmbedding(seq_length, in_dim, pos_embed_dim, activation)\n",
    "            if do_pos_embed\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i, (dim_block, do_res, do_attn) in enumerate(\n",
    "            zip(dim_blocks, res_blocks, attn_blocks)\n",
    "        ):\n",
    "            assert do_res or do_attn\n",
    "            block = nn.Module()\n",
    "            block.res = (\n",
    "                ResidualBlock(\n",
    "                    in_dim if i == 0 else dim_blocks[i - 1],\n",
    "                    dim_block,\n",
    "                    kernel_size,\n",
    "                    activation,\n",
    "                )\n",
    "                if do_res\n",
    "                else nn.Identity()\n",
    "            )\n",
    "            block.attn = (\n",
    "                AttentionBlock(\n",
    "                    dim_block,\n",
    "                    embed_dim,\n",
    "                    num_heads,\n",
    "                    feedforward_dim,\n",
    "                    activation,\n",
    "                    dropout,\n",
    "                )\n",
    "                if do_attn\n",
    "                else nn.Identity()\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Linear(seq_length * dim_blocks[-1], 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.res(x)\n",
    "            x = block.attn(x)\n",
    "        return self.out_block(x.flatten(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e5d32",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7967b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size, do_ddp, params):\n",
    "    if do_ddp:\n",
    "        ddp_setup(rank, world_size)\n",
    "        \n",
    "    model = Model(**params).to(rank)\n",
    "    if do_ddp:\n",
    "        model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = params.optimizer(model.parameters(), params.lr)\n",
    "\n",
    "    if params.log_wandb and rank == 0:\n",
    "        wandb.init(\n",
    "            project=\"Sleep Staging\",\n",
    "            config=params,\n",
    "        )\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        avg_loss = 0\n",
    "        avg_acc = 0\n",
    "        if params.log_wandb and rank == 0:\n",
    "            wandb.log(dict(epoch = epoch + 1))\n",
    "        for i, (x, y) in enumerate(bar := tqdm(train_loader, desc=(\n",
    "                                f\"Training | Epoch: {epoch + 1} | \"\n",
    "                                f\"Acc: {avg_acc:.2%} | \"\n",
    "                                f\"Loss: {avg_loss:.4f}\"))):\n",
    "            # Send to GPU (if available)\n",
    "            x = x.to(rank)\n",
    "            y = y.to(rank)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(x)\n",
    "            loss = params.criterion(pred, y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            acc = (pred.argmax(-1) == y).float().mean()\n",
    "            total_acc += acc\n",
    "            avg_acc += acc\n",
    "            \n",
    "            # Log\n",
    "            if (i + 1) % params.log_freq == 0:\n",
    "                avg_acc /= params.log_freq\n",
    "                avg_loss /= params.log_freq\n",
    "                if params.log_wandb:\n",
    "                    wandb.log(dict(\n",
    "                        train_loss = avg_loss,\n",
    "                        train_acc = avg_acc)\n",
    "                    )\n",
    "                bar.set_description(f\"Training | Epoch: {epoch + 1} | \"\n",
    "                                f\"Acc: {avg_acc:.2%} | \"\n",
    "                                f\"Loss: {avg_loss:.4f}\")\n",
    "                avg_loss = 0\n",
    "                avg_acc = 0\n",
    "        \n",
    "        if params.log_wandb:\n",
    "            wandb.log(dict(\n",
    "                train_epoch_loss = total_loss/len(train_loader),\n",
    "                train_epoch_acc = total_acc/len(train_loader))\n",
    "            )\n",
    "\n",
    "        # Test Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (x, y) in enumerate(tqdm(val_loader, desc=\"Testing\")):\n",
    "                # Send to GPU (if available)\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred = model(x)\n",
    "                loss = params.criterion(pred, y)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Calculate Accuracy\n",
    "                acc = (pred.argmax(-1) == y).float().mean()\n",
    "                total_acc += acc\n",
    "            if params.log_wandb:\n",
    "                wandb.log(dict(\n",
    "                    val_epoch_loss = total_loss/len(val_loader),\n",
    "                    val_epoch_acc = total_acc/len(val_loader))\n",
    "                )\n",
    "\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (x, y) in enumerate(tqdm(homemade_test_loader, desc=\"Testing\")):\n",
    "                # Send to GPU (if available)\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred = model(x)\n",
    "                loss = params.criterion(pred, y)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Calculate Accuracy\n",
    "                acc = (pred.argmax(-1) == y).float().mean()\n",
    "                total_acc += acc\n",
    "            if params.log_wandb:\n",
    "                wandb.log(dict(\n",
    "                    homemade_epoch_loss = total_loss/len(homemade_test_loader),\n",
    "                    homemade_epoch_acc = total_acc/len(homemade_test_loader))\n",
    "                )\n",
    "            \n",
    "        # Save Model\n",
    "        file_path = os.path.abspath(os.path.join(params.pretrained_model_dir, f'model_{epoch + 1}.pt'))\n",
    "        link_path = os.path.abspath(os.path.join(params.pretrained_model_dir, 'model.pt'))\n",
    "        torch.save(model, file_path)\n",
    "        try:\n",
    "            os.remove(link_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        os.symlink(file_path, link_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1f6dd",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(params.pretrained_model_dir, 'model.pt')).to(device)\n",
    "optimizer = params.optimizer(model.parameters(), params.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3468ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.ft_last_block = False\n",
    "if params.ft_last_block:\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True if \"out_block\" in name else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.log_wandb:\n",
    "    wandb.init(\n",
    "        project=\"Sleep Staging\",\n",
    "        config=params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75640eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() \n",
    "for epoch in range(params.num_epochs_ft):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    wandb.log(dict(epoch = epoch + 1))\n",
    "    for i, (x, y) in enumerate(bar := tqdm(homemade_train_loader, desc=(\n",
    "                               f\"Training | Epoch: {epoch + 1} | \"\n",
    "                               f\"Acc: {avg_acc:.2%} | \"\n",
    "                               f\"Loss: {avg_loss:.4f}\"))):\n",
    "        # Send to GPU (if available)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x)\n",
    "        loss = params.criterion(pred, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        acc = (pred.argmax(-1) == y).float().mean()\n",
    "        total_acc += acc\n",
    "        avg_acc += acc\n",
    "        \n",
    "        # Log\n",
    "        if (i + 1) % params.log_freq == 0:\n",
    "            avg_acc /= params.log_freq\n",
    "            avg_loss /= params.log_freq\n",
    "            if params.log_wandb:\n",
    "                wandb.log(dict(\n",
    "                    train_loss = avg_loss,\n",
    "                    train_acc = avg_acc)\n",
    "                )\n",
    "            bar.set_description(f\"Training | Epoch: {epoch + 1} | \"\n",
    "                               f\"Acc: {avg_acc:.2%} | \"\n",
    "                               f\"Loss: {avg_loss:.4f}\")\n",
    "            avg_loss = 0\n",
    "            avg_acc = 0\n",
    "    \n",
    "    if params.log_wandb:\n",
    "        wandb.log(dict(\n",
    "            train_epoch_loss = total_loss/len(homemade_train_loader),\n",
    "            train_epoch_acc = total_acc/len(homemade_train_loader))\n",
    "        )\n",
    "\n",
    "    # Test Phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (x, y) in enumerate(tqdm(homemade_test_loader, desc=\"Testing\")):\n",
    "            # Send to GPU (if available)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(x)\n",
    "            loss = params.criterion(pred, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            acc = (pred.argmax(-1) == y).float().mean()\n",
    "            total_acc += acc\n",
    "        if params.log_wandb:\n",
    "            wandb.log(dict(\n",
    "                homemade_epoch_loss = total_loss/len(homemade_test_loader),\n",
    "                homemade_epoch_acc = total_acc/len(homemade_test_loader))\n",
    "            )\n",
    "        \n",
    "    # Save Model\n",
    "    file_path = os.path.abspath(os.path.join(params.finetuned_model_dir, f'model_{epoch + 1}.pt'))\n",
    "    link_path = os.path.abspath(os.path.join(params.finetuned_model_dir, 'model.pt'))\n",
    "    torch.save(model, file_path)\n",
    "    try:\n",
    "        os.remove(link_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    os.symlink(file_path, link_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8afe992",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c996d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(params.pretrained_model_dir, 'model.pt')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Test Phase\n",
    "model.eval()\n",
    "loader = homemade_test_loader\n",
    "total_loss = 0\n",
    "total_acc = 0\n",
    "ys = []\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    t0 = time()\n",
    "    for i, (x, y) in enumerate(tqdm(loader, desc=\"Testing\")):\n",
    "        # Save ground truth for plotting\n",
    "        ys.append(y)\n",
    "\n",
    "        # Send to GPU (if available)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x)\n",
    "        \n",
    "        loss = params.criterion(pred, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        pred = pred.argmax(-1)\n",
    "        acc = (pred == y).float().mean()\n",
    "        total_acc += acc\n",
    "\n",
    "        # Save prediction for plotting\n",
    "        preds.append(pred.cpu())\n",
    "    # wandb.log(dict(\n",
    "    #     homemade_loss = total_loss/len(homemade_loader),\n",
    "    #     homemade_acc = total_acc/len(homemade_loader))\n",
    "    # )\n",
    "    t1 = time()\n",
    "    preds = torch.cat(preds)\n",
    "    ys = torch.cat(ys)\n",
    "\n",
    "print(f\"Loss: {total_loss/len(loader):4f}\\n\"\n",
    "      f\"Acc: {total_acc/len(loader):4f}\\n\"\n",
    "      f\"Time per pred: {(t1-t0)/len(loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = slice(0, 500)\n",
    "_, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "ax1.plot(ys[window])\n",
    "ax1.set_title(\"Ground Truth\")\n",
    "ax2.plot(preds[window])\n",
    "ax2.set_title(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ys == preds).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(homemade_test_loader))\n",
    "pred = model(x.to(device)).cpu()\n",
    "print(y[0].item())\n",
    "plt.plot(x[0,1,:].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "pred = model(x.to(device)).cpu()\n",
    "print(y[0].item())\n",
    "plt.plot(x[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca733f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_emg = np.load(\"data/homemade_recordings/EEG-EMG_preproc.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "window = slice(128*i, int(128*(i + 6)))\n",
    "for i, row in enumerate(eeg_emg):\n",
    "    print(\"Idx \", i)\n",
    "    plt.plot(row[window])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636c61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
